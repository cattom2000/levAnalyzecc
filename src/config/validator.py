"""
é…ç½®éªŒè¯æ¨¡å—
éªŒè¯ç³»ç»Ÿé…ç½®çš„å®Œæ•´æ€§å’Œæ­£ç¡®æ€§
"""

import os
from pathlib import Path
from typing import List, Tuple, Optional
from .config import get_config


class ConfigValidationError(Exception):
    """é…ç½®éªŒè¯é”™è¯¯"""
    pass


def validate_data_paths() -> List[str]:
    """éªŒè¯æ•°æ®æ–‡ä»¶è·¯å¾„"""
    config = get_config()
    errors = []

    # æ£€æŸ¥FINRAæ•°æ®æ–‡ä»¶
    finra_path = Path(config.data_sources.finra_data_path)
    if not finra_path.exists():
        errors.append(f"FINRAæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {finra_path}")
    elif not finra_path.suffix == '.csv':
        errors.append(f"FINRAæ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯: {finra_path} (æœŸæœ› .csv)")

    # æ£€æŸ¥VIXæ•°æ®æ–‡ä»¶
    vix_path = Path(config.data_sources.vix_data_path)
    if not vix_path.exists():
        errors.append(f"VIXæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {vix_path}")
    elif not vix_path.suffix == '.csv':
        errors.append(f"VIXæ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯: {vix_path} (æœŸæœ› .csv)")

    return errors


def validate_database_config() -> List[str]:
    """éªŒè¯æ•°æ®åº“é…ç½®"""
    config = get_config()
    errors = []

    # æ£€æŸ¥ç¼“å­˜ç›®å½•
    cache_path = Path(config.database.cache_db_path)
    cache_dir = cache_path.parent
    if not cache_dir.exists():
        try:
            cache_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            errors.append(f"æ— æ³•åˆ›å»ºç¼“å­˜ç›®å½•: {cache_dir} - {e}")

    # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æƒé™
    if cache_path.exists() and not os.access(cache_path, os.W_OK):
        errors.append(f"ç¼“å­˜æ•°æ®åº“æ–‡ä»¶æ— å†™æƒé™: {cache_path}")

    return errors


def validate_analysis_config() -> List[str]:
    """éªŒè¯åˆ†æé…ç½®"""
    config = get_config()
    errors = []

    # éªŒè¯é˜ˆå€¼èŒƒå›´
    if not 0 < config.analysis.leverage_warning_threshold <= 1:
        errors.append(f"æ æ†é£é™©é˜ˆå€¼åº”åœ¨0-1ä¹‹é—´: {config.analysis.leverage_warning_threshold}")

    if config.analysis.growth_warning_upper <= config.analysis.growth_warning_lower:
        errors.append(f"å¢é•¿ç‡è­¦å‘Šä¸Šé™åº”å¤§äºä¸‹é™: {config.analysis.growth_warning_upper} <= {config.analysis.growth_warning_lower}")

    if config.analysis.fragility_healthy_range[0] >= config.analysis.fragility_healthy_range[1]:
        errors.append(f"è„†å¼±æ€§å¥åº·åŒºé—´èŒƒå›´æ— æ•ˆ: {config.analysis.fragility_healthy_range}")

    # éªŒè¯Z-scoreçª—å£
    if config.analysis.zscore_window_months < 12:
        errors.append(f"Z-scoreè®¡ç®—çª—å£åº”è‡³å°‘12ä¸ªæœˆ: {config.analysis.zscore_window_months}")

    return errors


def validate_system_config() -> List[str]:
    """éªŒè¯ç³»ç»Ÿé…ç½®"""
    config = get_config()
    errors = []

    # éªŒè¯æ—¥å¿—çº§åˆ«
    valid_log_levels = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']
    if config.system.log_level.upper() not in valid_log_levels:
        errors.append(f"æ— æ•ˆçš„æ—¥å¿—çº§åˆ«: {config.system.log_level} (æœ‰æ•ˆå€¼: {valid_log_levels})")

    # éªŒè¯æ€§èƒ½é…ç½®
    if config.system.max_concurrent_requests < 1:
        errors.append(f"æœ€å¤§å¹¶å‘è¯·æ±‚æ•°åº”å¤§äº0: {config.system.max_concurrent_requests}")

    if config.system.request_timeout_seconds < 1:
        errors.append(f"è¯·æ±‚è¶…æ—¶æ—¶é—´åº”å¤§äº0ç§’: {config.system.request_timeout_seconds}")

    return errors


def validate_dependencies() -> List[str]:
    """éªŒè¯ä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…"""
    required_packages = [
        'pandas', 'numpy', 'streamlit', 'plotly', 'yfinance',
        'scipy', 'scikit-learn', 'statsmodels', 'requests'
    ]

    errors = []
    for package in required_packages:
        try:
            if package == 'scikit-learn':
                __import__('sklearn')
            else:
                __import__(package)
        except ImportError:
            errors.append(f"ç¼ºå°‘å¿…éœ€çš„PythonåŒ…: {package}")

    return errors


def validate_all() -> Tuple[bool, List[str]]:
    """æ‰§è¡Œæ‰€æœ‰é…ç½®éªŒè¯"""
    all_errors = []

    # æ‰§è¡Œå„é¡¹éªŒè¯
    all_errors.extend(validate_data_paths())
    all_errors.extend(validate_database_config())
    all_errors.extend(validate_analysis_config())
    all_errors.extend(validate_system_config())
    all_errors.extend(validate_dependencies())

    return len(all_errors) == 0, all_errors


def get_validation_report() -> str:
    """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
    is_valid, errors = validate_all()

    if is_valid:
        return "âœ… é…ç½®éªŒè¯é€šè¿‡"

    report = "âŒ é…ç½®éªŒè¯å¤±è´¥:\n"
    for i, error in enumerate(errors, 1):
        report += f"{i}. {error}\n"

    return report


def check_data_quality() -> dict:
    """æ£€æŸ¥æ•°æ®è´¨é‡"""
    config = get_config()
    results = {}

    # æ£€æŸ¥FINRAæ•°æ®
    finra_path = Path(config.data_sources.finra_data_path)
    if finra_path.exists():
        try:
            import pandas as pd
            df = pd.read_csv(finra_path)
            results['finra'] = {
                'exists': True,
                'rows': len(df),
                'columns': len(df.columns),
                'file_size_mb': finra_path.stat().st_size / (1024 * 1024),
                'last_modified': finra_path.stat().st_mtime
            }
        except Exception as e:
            results['finra'] = {'exists': True, 'error': str(e)}
    else:
        results['finra'] = {'exists': False}

    # æ£€æŸ¥VIXæ•°æ®
    vix_path = Path(config.data_sources.vix_data_path)
    if vix_path.exists():
        try:
            import pandas as pd
            df = pd.read_csv(vix_path)
            results['vix'] = {
                'exists': True,
                'rows': len(df),
                'columns': len(df.columns),
                'file_size_mb': vix_path.stat().st_size / (1024 * 1024),
                'last_modified': vix_path.stat().st_mtime
            }
        except Exception as e:
            results['vix'] = {'exists': True, 'error': str(e)}
    else:
        results['vix'] = {'exists': False}

    return results


if __name__ == "__main__":
    # è¿è¡ŒéªŒè¯
    print("ğŸ” é…ç½®éªŒè¯æŠ¥å‘Š")
    print("=" * 50)
    print(get_validation_report())

    print("\nğŸ“Š æ•°æ®è´¨é‡æ£€æŸ¥")
    print("=" * 50)
    data_quality = check_data_quality()
    for source, info in data_quality.items():
        print(f"\n{source.upper()}:")
        for key, value in info.items():
            print(f"  {key}: {value}")