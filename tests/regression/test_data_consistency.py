"""
æ•°æ®ä¸€è‡´æ€§éªŒè¯æµ‹è¯• - ç¡®ä¿è·¨ç»„ä»¶ã€è·¨æ—¶é—´çš„æ•°æ®ä¸€è‡´æ€§
"""

import pytest
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import hashlib
import json
from pathlib import Path
import tempfile

from src.analysis.calculators.leverage_ratio_calculator import LeverageRatioCalculator
from src.analysis.calculators.money_supply_calculator import MoneySupplyCalculator
from src.analysis.calculators.fragility_calculator import FragilityCalculator
from src.data.collectors.finra_collector import FINRACollector
from src.data.validators.base_validator import FinancialDataValidator


class TestDataConsistency:
    """æµ‹è¯•å¥—ä»¶ï¼šæ•°æ®ä¸€è‡´æ€§éªŒè¯"""

    @pytest.fixture
    def consistent_test_data(self):
        """åˆ›å»ºä¸€è‡´æ€§æµ‹è¯•çš„æ•°æ®é›†"""
        np.random.seed(99999)  # å›ºå®šç§å­ç¡®ä¿å¯é‡å¤æ€§

        dates = pd.date_range("2020-01-31", periods=36, freq="M")  # 3å¹´æœˆåº¦æ•°æ®

        # åˆ›å»ºåŸºç¡€å¸‚åœºæ•°æ®
        base_market_cap = np.random.uniform(1e12, 5e12, 36)
        leverage_ratios = np.random.uniform(0.1, 0.25, 36)

        # ç¡®ä¿æ•°æ®å†…éƒ¨ä¸€è‡´æ€§
        debit_balances = leverage_ratios * base_market_cap
        account_count = np.random.randint(50000, 200000, 36)
        firm_count = np.random.randint(100, 500, 36)

        # M2è´§å¸ä¾›åº”æ•°æ®ï¼ˆä¸å€ºåŠ¡ä½™é¢ä¿æŒåˆç†å…³ç³»ï¼‰
        m2_supply = np.random.uniform(15e12, 20e12, 36)

        # å¸‚åœºæŒ‡æ•°æ•°æ®ï¼ˆä¸æ æ†ç‡æœ‰åˆç†ç›¸å…³æ€§ï¼‰
        sp500_levels = np.random.uniform(3000, 4500, 36)
        vix_levels = 50 - leverage_ratios * 100  # è´Ÿç›¸å…³ï¼šé«˜æ æ†é€šå¸¸å¯¹åº”ä½VIX

        return pd.DataFrame(
            {
                "date": dates,
                "debit_balances": debit_balances,
                "market_cap": base_market_cap,
                "leverage_ratio": leverage_ratios,
                "account_count": account_count,
                "firm_count": firm_count,
                "m2_money_supply": m2_supply,
                "sp500_level": sp500_levels,
                "vix_level": vix_levels,
                "unemployment_rate": np.random.uniform(3.0, 8.0, 36),
                "gdp_growth_rate": np.random.uniform(-0.05, 0.08, 36),
            }
        )

    def test_internal_data_consistency(self, consistent_test_data):
        """æµ‹è¯•æ•°æ®å†…éƒ¨ä¸€è‡´æ€§"""
        data = consistent_test_data.copy()

        consistency_issues = []

        # æ£€æŸ¥æ æ†ç‡è®¡ç®—ä¸€è‡´æ€§
        calculated_leverage = data["debit_balances"] / data["market_cap"]
        leverage_difference = np.abs(data["leverage_ratio"] - calculated_leverage)
        max_leverage_diff = leverage_difference.max()

        if max_leverage_diff > 1e-10:
            consistency_issues.append(f"æ æ†ç‡è®¡ç®—ä¸ä¸€è‡´ï¼Œæœ€å¤§å·®å¼‚: {max_leverage_diff}")

        # æ£€æŸ¥è´¢åŠ¡æ•°æ®åˆç†æ€§
        # å€ºåŠ¡ä½™é¢åº”è¯¥å°äºå¸‚å€¼ï¼ˆé€šå¸¸æƒ…å†µä¸‹ï¼‰
        invalid_leverage_periods = data[data["debit_balances"] > data["market_cap"]]
        if len(invalid_leverage_periods) > 0:
            consistency_issues.append(
                f"å‘ç° {len(invalid_leverage_periods)} ä¸ªå€ºåŠ¡ä½™é¢è¶…è¿‡å¸‚å€¼çš„æ—¶æœŸ"
            )

        # æ£€æŸ¥è´¦æˆ·æ•°é‡å’Œå…¬å¸æ•°é‡çš„å…³ç³»
        invalid_accounts = data[data["account_count"] < data["firm_count"]]
        if len(invalid_accounts) > 0:
            consistency_issues.append(f"å‘ç° {len(invalid_accounts)} ä¸ªè´¦æˆ·æ•°å°‘äºå…¬å¸æ•°çš„æ—¶æœŸ")

        # æ£€æŸ¥VIXå’Œæ æ†ç‡çš„è´Ÿç›¸å…³æ€§
        correlation = data["leverage_ratio"].corr(data["vix_level"])
        if correlation > -0.1:  # åº”è¯¥æœ‰è´Ÿç›¸å…³
            consistency_issues.append(f"VIXä¸æ æ†ç‡çš„è´Ÿç›¸å…³æ€§å¼‚å¸¸: {correlation:.3f}")

        # æ£€æŸ¥æ•°æ®èŒƒå›´åˆç†æ€§
        if data["leverage_ratio"].min() < 0 or data["leverage_ratio"].max() > 1:
            consistency_issues.append(
                f"æ æ†ç‡è¶…å‡ºåˆç†èŒƒå›´: [{data['leverage_ratio'].min():.3f}, {data['leverage_ratio'].max():.3f}]"
            )

        if data["unemployment_rate"].min() < 0 or data["unemployment_rate"].max() > 25:
            consistency_issues.append(
                f"å¤±ä¸šç‡è¶…å‡ºåˆç†èŒƒå›´: [{data['unemployment_rate'].min():.1f}%, {data['unemployment_rate'].max():.1f}%]"
            )

        print(f"å†…éƒ¨æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥:")
        if consistency_issues:
            for issue in consistency_issues:
                print(f"  âŒ {issue}")
            pytest.fail(f"å‘ç° {len(consistency_issues)} ä¸ªæ•°æ®ä¸€è‡´æ€§é—®é¢˜")
        else:
            print("  âœ… æ‰€æœ‰å†…éƒ¨ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")

    def test_cross_component_calculation_consistency(self, consistent_test_data):
        """æµ‹è¯•è·¨ç»„ä»¶è®¡ç®—ä¸€è‡´æ€§"""
        data = consistent_test_data.copy()

        leverage_calc = LeverageRatioCalculator()
        money_supply_calc = MoneySupplyCalculator()

        # ä½¿ç”¨ä¸åŒæ–¹æ³•è®¡ç®—ç›¸åŒçš„æŒ‡æ ‡
        methods_results = {}

        # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨é¢„è®¡ç®—çš„æ æ†ç‡
        method1_ratios = data["leverage_ratio"]

        # æ–¹æ³•2: ä»å€ºåŠ¡å’Œå¸‚å€¼é‡æ–°è®¡ç®—
        leverage_data = data[["debit_balances", "market_cap"]]
        method2_ratios = leverage_calc._calculate_leverage_ratio(leverage_data)

        # æ–¹æ³•3: åˆ†æ‰¹è®¡ç®—ç„¶ååˆå¹¶
        batch_size = 12
        batch_results = []
        for i in range(0, len(data), batch_size):
            batch_data = data.iloc[i : i + batch_size][["debit_balances", "market_cap"]]
            batch_result = leverage_calc._calculate_leverage_ratio(batch_data)
            batch_results.append(batch_result)
        method3_ratios = pd.concat(batch_results, ignore_index=True)

        # æ¯”è¾ƒä¸åŒæ–¹æ³•çš„ç»“æœ
        comparisons = {
            "method1_vs_method2": np.allclose(
                method1_ratios, method2_ratios, rtol=1e-10
            ),
            "method1_vs_method3": np.allclose(
                method1_ratios, method3_ratios, rtol=1e-10
            ),
            "method2_vs_method3": np.allclose(
                method2_ratios, method3_ratios, rtol=1e-10
            ),
        }

        # è®¡ç®—å·®å¼‚ç»Ÿè®¡
        differences = {
            "method1_method2_max_diff": np.abs(method1_ratios - method2_ratios).max(),
            "method1_method3_max_diff": np.abs(method1_ratios - method3_ratios).max(),
            "method2_method3_max_diff": np.abs(method2_ratios - method3_ratios).max(),
        }

        print(f"è·¨ç»„ä»¶è®¡ç®—ä¸€è‡´æ€§æ£€æŸ¥:")
        for comparison, is_consistent in comparisons.items():
            status = "âœ…" if is_consistent else "âŒ"
            print(f"  {status} {comparison}: {is_consistent}")

        for diff_name, max_diff in differences.items():
            print(f"  ğŸ“Š {diff_name}: {max_diff:.2e}")

        # æ–­è¨€æ‰€æœ‰æ–¹æ³•åº”è¯¥äº§ç”Ÿä¸€è‡´çš„ç»“æœ
        assert all(comparisons.values()), "è·¨ç»„ä»¶è®¡ç®—ç»“æœä¸ä¸€è‡´"

        # æ–­è¨€å·®å¼‚åº”è¯¥éå¸¸å°
        for diff_name, max_diff in differences.items():
            assert max_diff < 1e-9, f"{diff_name} å·®å¼‚è¿‡å¤§: {max_diff}"

    def test_time_series_consistency(self, consistent_test_data):
        """æµ‹è¯•æ—¶é—´åºåˆ—ä¸€è‡´æ€§"""
        data = consistent_test_data.copy()

        # æ£€æŸ¥æ—¶é—´åºåˆ—çš„è¿ç»­æ€§
        date_gaps = data["date"].diff().dropna()
        expected_frequency = pd.Timedelta(days=30)  # æœˆåº¦æ•°æ®

        # å…è®¸ä¸€äº›è¯¯å·®ï¼ˆæœˆä»½å¤©æ•°å·®å¼‚ï¼‰
        tolerance_days = 5
        inconsistent_gaps = date_gaps[
            abs(date_gaps - expected_frequency) > pd.Timedelta(days=tolerance_days)
        ]

        consistency_issues = []

        if len(inconsistent_gaps) > 0:
            consistency_issues.append(f"å‘ç° {len(inconsistent_gaps)} ä¸ªä¸ä¸€è‡´çš„æ—¶é—´é—´éš”")

        # æ£€æŸ¥æ—¶é—´åºåˆ—çš„å•è°ƒæ€§
        if not data["date"].is_monotonic_increasing:
            consistency_issues.append("æ—¥æœŸä¸æ˜¯å•è°ƒé€’å¢çš„")

        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æ—¥æœŸ
        if not data["date"].is_unique:
            consistency_issues.append("å­˜åœ¨é‡å¤çš„æ—¥æœŸ")

        # æ£€æŸ¥è´¢åŠ¡æŒ‡æ ‡çš„æ—¶åºåˆç†æ€§
        # æ æ†ç‡ä¸åº”è¯¥æœ‰å¼‚å¸¸å¤§çš„è·³è·ƒ
        leverage_changes = data["leverage_ratio"].diff().abs()
        extreme_changes = leverage_changes[leverage_changes > 0.1]  # 10%ä»¥ä¸Šçš„å˜åŒ–

        if len(extreme_changes) > 0:
            consistency_issues.append(f"å‘ç° {len(extreme_changes)} ä¸ªæ æ†ç‡å¼‚å¸¸å˜åŒ–")

        # æ£€æŸ¥å­£èŠ‚æ€§æ¨¡å¼çš„åˆç†æ€§
        monthly_leverage = data.groupby(data["date"].dt.month)["leverage_ratio"].mean()
        leverage_seasonality_std = monthly_leverage.std()

        if leverage_seasonality_std > 0.05:  # å­£èŠ‚æ€§æ ‡å‡†å·®è¿‡å¤§
            consistency_issues.append(f"æ æ†ç‡å­£èŠ‚æ€§æ³¢åŠ¨è¿‡å¤§: {leverage_seasonality_std:.3f}")

        print(f"æ—¶é—´åºåˆ—ä¸€è‡´æ€§æ£€æŸ¥:")
        if consistency_issues:
            for issue in consistency_issues:
                print(f"  âŒ {issue}")
            pytest.fail(f"å‘ç° {len(consistency_issues)} ä¸ªæ—¶é—´åºåˆ—ä¸€è‡´æ€§é—®é¢˜")
        else:
            print("  âœ… æ‰€æœ‰æ—¶é—´åºåˆ—ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")

    def test_cross_source_data_consistency(self, consistent_test_data):
        """æµ‹è¯•è·¨æ•°æ®æºä¸€è‡´æ€§"""
        data = consistent_test_data.copy()

        # æ¨¡æ‹Ÿä»ä¸åŒæ•°æ®æºè·å–çš„ç›¸åŒæŒ‡æ ‡
        source_discrepancies = {}

        # æ¨¡æ‹ŸFINRAæ•°æ®æºçš„è®¡ç®—
        finra_calculated_ratios = data["debit_balances"] / data["market_cap"]

        # æ¨¡æ‹Ÿç¬¬ä¸‰æ–¹æ•°æ®æºçš„æ æ†ç‡ï¼ˆæ·»åŠ å°å¹…å™ªå£°æ¨¡æ‹Ÿå·®å¼‚ï¼‰
        np.random.seed(42)
        third_party_noise = np.random.normal(0, 0.001, len(data))  # 0.1%çš„å™ªå£°
        third_party_ratios = data["leverage_ratio"] + third_party_noise

        # è®¡ç®—æ•°æ®æºä¹‹é—´çš„å·®å¼‚
        finra_diff = np.abs(finra_calculated_ratios - data["leverage_ratio"])
        third_party_diff = np.abs(third_party_ratios - data["leverage_ratio"])

        source_discrepancies = {
            "finra_vs_calculated_max_diff": finra_diff.max(),
            "finra_vs_calculated_mean_diff": finra_diff.mean(),
            "third_party_max_diff": third_party_diff.max(),
            "third_party_mean_diff": third_party_diff.mean(),
            "third_party_outliers": len(
                third_party_diff[third_party_diff > 0.01]
            ),  # è¶…è¿‡1%çš„å·®å¼‚
        }

        print(f"è·¨æ•°æ®æºä¸€è‡´æ€§æ£€æŸ¥:")
        for metric, value in source_discrepancies.items():
            print(f"  ğŸ“Š {metric}: {value:.6f}")

        # éªŒè¯æ•°æ®æºä¸€è‡´æ€§
        # FINRAè®¡ç®—åº”è¯¥å®Œå…¨ä¸€è‡´
        assert (
            source_discrepancies["finra_vs_calculated_max_diff"] < 1e-10
        ), "FINRAè®¡ç®—ä¸é¢„è®¡ç®—å€¼ä¸ä¸€è‡´"

        # ç¬¬ä¸‰æ–¹æ•°æ®å·®å¼‚åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
        assert source_discrepancies["third_party_max_diff"] < 0.02, "ç¬¬ä¸‰æ–¹æ•°æ®å·®å¼‚è¿‡å¤§"
        assert (
            source_discrepancies["third_party_outliers"] < len(data) * 0.05
        ), "ç¬¬ä¸‰æ–¹æ•°æ®å¼‚å¸¸å€¼è¿‡å¤š"

    def test_statistical_consistency(self, consistent_test_data):
        """æµ‹è¯•ç»Ÿè®¡ä¸€è‡´æ€§"""
        data = consistent_test_data.copy()

        leverage_calc = LeverageRatioCalculator()

        # ä½¿ç”¨ä¸åŒæ–¹æ³•è®¡ç®—ç»Ÿè®¡é‡
        leverage_ratios = data["leverage_ratio"]

        # æ–¹æ³•1: ä½¿ç”¨pandaså†…ç½®å‡½æ•°
        pandas_stats = {
            "mean": leverage_ratios.mean(),
            "std": leverage_ratios.std(),
            "min": leverage_ratios.min(),
            "max": leverage_ratios.max(),
            "median": leverage_ratios.median(),
            "q25": leverage_ratios.quantile(0.25),
            "q75": leverage_ratios.quantile(0.75),
        }

        # æ–¹æ³•2: ä½¿ç”¨numpy
        numpy_stats = {
            "mean": np.mean(leverage_ratios),
            "std": np.std(leverage_ratios),
            "min": np.min(leverage_ratios),
            "max": np.max(leverage_ratios),
            "median": np.median(leverage_ratios),
            "q25": np.percentile(leverage_ratios, 25),
            "q75": np.percentile(leverage_ratios, 75),
        }

        # æ–¹æ³•3: ä½¿ç”¨è‡ªå®šä¹‰è®¡ç®—å™¨
        calculator_stats = leverage_calc._calculate_leverage_statistics(leverage_ratios)

        # æ¯”è¾ƒç»Ÿè®¡é‡çš„ä¸€è‡´æ€§
        statistical_consistency = {}
        tolerance = 1e-10

        for stat_name in pandas_stats.keys():
            pandas_val = pandas_stats[stat_name]
            numpy_val = numpy_stats[stat_name]
            calculator_val = calculator_stats.get(stat_name, None)

            pandas_numpy_diff = abs(pandas_val - numpy_val)

            if calculator_val is not None:
                pandas_calculator_diff = abs(pandas_val - calculator_val)
                numpy_calculator_diff = abs(numpy_val - calculator_val)

                statistical_consistency[stat_name] = {
                    "pandas_numpy_consistent": pandas_numpy_diff < tolerance,
                    "pandas_calculator_consistent": pandas_calculator_diff < tolerance,
                    "numpy_calculator_consistent": numpy_calculator_diff < tolerance,
                    "max_diff": max(
                        pandas_numpy_diff, pandas_calculator_diff, numpy_calculator_diff
                    ),
                }
            else:
                statistical_consistency[stat_name] = {
                    "pandas_numpy_consistent": pandas_numpy_diff < tolerance,
                    "max_diff": pandas_numpy_diff,
                }

        print(f"ç»Ÿè®¡ä¸€è‡´æ€§æ£€æŸ¥:")
        all_consistent = True

        for stat_name, consistency in statistical_consistency.items():
            consistent_indicators = []
            for key, value in consistency.items():
                if key.endswith("_consistent") and isinstance(value, bool):
                    status = "âœ…" if value else "âŒ"
                    consistent_indicators.append(f"{status}")
                    if not value:
                        all_consistent = False

            max_diff = consistency.get("max_diff", 0)
            print(
                f"  {stat_name}: {' '.join(consistent_indicators)} æœ€å¤§å·®å¼‚: {max_diff:.2e}"
            )

        assert all_consistent, "ç»Ÿè®¡è®¡ç®—ä¸ä¸€è‡´"

    def test_data_format_consistency(self, consistent_test_data):
        """æµ‹è¯•æ•°æ®æ ¼å¼ä¸€è‡´æ€§"""
        data = consistent_test_data.copy()

        format_consistency_issues = []

        # æ£€æŸ¥æ•°æ®ç±»å‹
        expected_types = {
            "date": "datetime64[ns]",
            "debit_balances": "float64",
            "market_cap": "float64",
            "leverage_ratio": "float64",
            "account_count": "int64",
            "firm_count": "int64",
            "m2_money_supply": "float64",
            "sp500_level": "float64",
            "vix_level": "float64",
            "unemployment_rate": "float64",
            "gdp_growth_rate": "float64",
        }

        for column, expected_type in expected_types.items():
            if column in data.columns:
                actual_type = str(data[column].dtype)
                if actual_type != expected_type:
                    format_consistency_issues.append(
                        f"åˆ— {column} ç±»å‹ä¸åŒ¹é…: æœŸæœ› {expected_type}, å®é™… {actual_type}"
                    )
            else:
                format_consistency_issues.append(f"ç¼ºå°‘å¿…éœ€åˆ—: {column}")

        # æ£€æŸ¥æ—¥æœŸæ ¼å¼
        if "date" in data.columns:
            if not pd.api.types.is_datetime64_any_dtype(data["date"]):
                format_consistency_issues.append("æ—¥æœŸåˆ—ä¸æ˜¯datetimeç±»å‹")

            # æ£€æŸ¥æ—¥æœŸèŒƒå›´åˆç†æ€§
            min_date = data["date"].min()
            max_date = data["date"].max()

            if min_date < pd.Timestamp("2000-01-01"):
                format_consistency_issues.append(f"æœ€æ—©æ—¥æœŸè¿‡æ—§: {min_date}")

            if max_date > pd.Timestamp("2030-12-31"):
                format_consistency_issues.append(f"æœ€æ–°æ—¥æœŸè¿‡æ–°: {max_date}")

        # æ£€æŸ¥æ•°å€¼åˆ—çš„åˆç†æ€§
        numeric_columns = [
            "debit_balances",
            "market_cap",
            "leverage_ratio",
            "m2_money_supply",
            "sp500_level",
            "vix_level",
        ]

        for col in numeric_columns:
            if col in data.columns:
                # æ£€æŸ¥NaNå€¼
                nan_count = data[col].isna().sum()
                if nan_count > 0:
                    format_consistency_issues.append(f"åˆ— {col} åŒ…å« {nan_count} ä¸ªNaNå€¼")

                # æ£€æŸ¥æ— ç©·å€¼
                inf_count = np.isinf(data[col]).sum()
                if inf_count > 0:
                    format_consistency_issues.append(f"åˆ— {col} åŒ…å« {inf_count} ä¸ªæ— ç©·å€¼")

                # æ£€æŸ¥è´Ÿå€¼ï¼ˆå¯¹äºä¸åº”è¯¥ä¸ºè´Ÿçš„åˆ—ï¼‰
                if col in [
                    "debit_balances",
                    "market_cap",
                    "m2_money_supply",
                    "sp500_level",
                    "account_count",
                    "firm_count",
                ]:
                    negative_count = (data[col] < 0).sum()
                    if negative_count > 0:
                        format_consistency_issues.append(
                            f"åˆ— {col} åŒ…å« {negative_count} ä¸ªè´Ÿå€¼"
                        )

        print(f"æ•°æ®æ ¼å¼ä¸€è‡´æ€§æ£€æŸ¥:")
        if format_consistency_issues:
            for issue in format_consistency_issues:
                print(f"  âŒ {issue}")
            pytest.fail(f"å‘ç° {len(format_consistency_issues)} ä¸ªæ•°æ®æ ¼å¼ä¸€è‡´æ€§é—®é¢˜")
        else:
            print("  âœ… æ‰€æœ‰æ•°æ®æ ¼å¼ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")

    def test_data_integrity_hash_consistency(self, consistent_test_data):
        """æµ‹è¯•æ•°æ®å®Œæ•´æ€§å“ˆå¸Œä¸€è‡´æ€§"""
        data = consistent_test_data.copy()

        # è®¡ç®—æ•°æ®å“ˆå¸Œ
        def calculate_dataframe_hash(df):
            """è®¡ç®—DataFrameçš„å“ˆå¸Œå€¼"""
            # æ’åºä»¥ç¡®ä¿ä¸€è‡´æ€§
            df_sorted = df.sort_values(by="date").reset_index(drop=True)
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶è®¡ç®—å“ˆå¸Œ
            data_string = df_sorted.to_string()
            return hashlib.md5(data_string.encode()).hexdigest()

        # åŸå§‹æ•°æ®å“ˆå¸Œ
        original_hash = calculate_dataframe_hash(data)

        # åˆ›å»ºæ•°æ®å‰¯æœ¬å¹¶éªŒè¯å“ˆå¸Œä¸€è‡´æ€§
        data_copy = data.copy()
        copy_hash = calculate_dataframe_hash(data_copy)

        # åˆ›å»ºä¸åŒçš„æ•°æ®é¡ºåºï¼ˆåº”è¯¥äº§ç”Ÿä¸åŒçš„å“ˆå¸Œï¼‰
        data_shuffled = data.sample(frac=1, random_state=42)
        shuffled_hash = calculate_dataframe_hash(data_shuffled)

        # åˆ›å»ºå°å¹…ä¿®æ”¹çš„æ•°æ®
        data_modified = data.copy()
        data_modified.loc[0, "leverage_ratio"] += 0.0001
        modified_hash = calculate_dataframe_hash(data_modified)

        print(f"æ•°æ®å®Œæ•´æ€§å“ˆå¸Œæ£€æŸ¥:")
        print(f"  åŸå§‹æ•°æ®å“ˆå¸Œ: {original_hash}")
        print(f"  å‰¯æœ¬æ•°æ®å“ˆå¸Œ: {copy_hash}")
        print(f"  æ‰“ä¹±æ•°æ®å“ˆå¸Œ: {shuffled_hash}")
        print(f"  ä¿®æ”¹æ•°æ®å“ˆå¸Œ: {modified_hash}")

        # éªŒè¯å“ˆå¸Œä¸€è‡´æ€§
        assert original_hash == copy_hash, "ç›¸åŒæ•°æ®çš„å“ˆå¸Œåº”è¯¥ä¸€è‡´"
        assert original_hash != shuffled_hash, "ä¸åŒé¡ºåºçš„æ•°æ®åº”è¯¥äº§ç”Ÿä¸åŒå“ˆå¸Œ"
        assert original_hash != modified_hash, "ä¿®æ”¹è¿‡çš„æ•°æ®åº”è¯¥äº§ç”Ÿä¸åŒå“ˆå¸Œ"

        # éªŒè¯å“ˆå¸Œçš„å”¯ä¸€æ€§
        all_hashes = [original_hash, copy_hash, shuffled_hash, modified_hash]
        unique_hashes = len(set(all_hashes))

        print(f"  å“ˆå¸Œå”¯ä¸€æ€§: {unique_hashes}/{len(all_hashes)} ä¸ªå”¯ä¸€å“ˆå¸Œ")
        assert unique_hashes >= 3, "åº”è¯¥æœ‰è‡³å°‘3ä¸ªä¸åŒçš„å“ˆå¸Œå€¼"

    def test_cross_validation_consistency(self, consistent_test_data):
        """æµ‹è¯•äº¤å‰éªŒè¯ä¸€è‡´æ€§"""
        data = consistent_test_data.copy()

        leverage_calc = LeverageRatioCalculator()

        # æ‰§è¡Œä¸åŒè§„æ¨¡çš„äº¤å‰éªŒè¯
        validation_sizes = [20, 25, 30]  # ä¸åŒçš„è®­ç»ƒé›†å¤§å°
        validation_results = {}

        for train_size in validation_sizes:
            # åˆ†å‰²æ•°æ®
            train_data = data.iloc[:train_size]
            test_data = data.iloc[train_size:]

            # åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—ç»Ÿè®¡é‡
            train_stats = leverage_calc._calculate_leverage_statistics(
                train_data["leverage_ratio"]
            )

            # éªŒè¯æµ‹è¯•é›†æ•°æ®æ˜¯å¦åœ¨è®­ç»ƒé›†çš„åˆç†èŒƒå›´å†…
            test_ratios = test_data["leverage_ratio"]

            # è®¡ç®—Z-score
            z_scores = [
                (ratio - train_stats["mean"]) / train_stats["std"]
                if train_stats["std"] > 0
                else 0
                for ratio in test_ratios
            ]

            # ç»Ÿè®¡å¼‚å¸¸å€¼
            outliers = [z for z in z_scores if abs(z) > 2]  # 2å€æ ‡å‡†å·®
            outlier_rate = len(outliers) / len(test_ratios)

            # è®¡ç®—é¢„æµ‹è¯¯å·®
            predicted_ratios = [train_stats["mean"]] * len(test_ratios)  # ç®€å•é¢„æµ‹
            mae = np.mean(np.abs(test_ratios - predicted_ratios))

            validation_results[train_size] = {
                "outlier_rate": outlier_rate,
                "max_z_score": max(abs(z) for z in z_scores),
                "mae": mae,
                "train_size": train_size,
                "test_size": len(test_data),
            }

        print(f"äº¤å‰éªŒè¯ä¸€è‡´æ€§æ£€æŸ¥:")
        for train_size, results in validation_results.items():
            print(
                f"  è®­ç»ƒå¤§å° {train_size}: å¼‚å¸¸ç‡ {results['outlier_rate']:.2%}, "
                f"æœ€å¤§Zåˆ†æ•° {results['max_z_score']:.2f}, MAE {results['mae']:.4f}"
            )

        # éªŒè¯äº¤å‰éªŒè¯çš„ä¸€è‡´æ€§
        outlier_rates = [
            results["outlier_rate"] for results in validation_results.values()
        ]
        max_outlier_rate = max(outlier_rates)

        # å¼‚å¸¸ç‡åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
        assert max_outlier_rate < 0.5, f"äº¤å‰éªŒè¯å¼‚å¸¸ç‡è¿‡é«˜: {max_outlier_rate:.2%}"

        # ä¸åŒè®­ç»ƒé›†å¤§å°åº”è¯¥äº§ç”Ÿç›¸å¯¹ä¸€è‡´çš„ç»“æœ
        outlier_rate_std = np.std(outlier_rates)
        assert outlier_rate_std < 0.2, f"äº¤å‰éªŒè¯ç»“æœä¸ç¨³å®šï¼Œå¼‚å¸¸ç‡æ ‡å‡†å·®: {outlier_rate_std:.2%}"
